rdb {
  url = "jdbc:mysql://rest-db:3306/sight"
  user = "sight"
  password = "%oEL!L#Lkf&B!$F9JapY"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  jdbc.url = "jdbc:hive2://master:10000/default"
}
message.client.url="http://rest:5555/"
kylin.client.url="http://node14:7070/kylin/api/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="kafka001:6667,kafka002:6667,kafka003:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "kafka001:6667,kafka002:6667,kafka003:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    //auto.offset.reset = "earliest"
    //nadx集群有问题，只能用latest
    auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  set {
    hbase.zookeeper.quorum = "master"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  //  app.name = "nadx_dwi"
  streaming.batch.buration = 100
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
    //    spark.streaming.kafka.maxRatePerPartition = 2000
    spark.streaming.kafka.maxRatePerPartition = 5000
    #spark.streaming.receiver.maxRate=1000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 5
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 5
    spark.kryoserializer.buffer=64m
    spark.kryoserializer.buffer.max=512m
    spark.scheduler.mode=FAIR
    spark.history.fs.cleaner.enabled = true
    spark.history.fs.cleaner.interval = 1d
    spark.history.fs.cleaner.maxAge = 3d
  }
}
modules {
  sdk_dyn_traffic {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    master = true
    b_time.by = "from_unixtime(timestamp/1000, 'yyyy-MM-dd HH:mm:ss')"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SdkDynLoadingTrafficDWISchema"
    dwr.enable = true
    dwr.groupby.fields = [
      { expr = "adv",       as = "adv"      },
      { expr = "adv_bd",    as = "adv_bd"   },
      { expr = "jar",       as = "jar"      },
      { expr = "pub",       as = "pub"      },
      { expr = "pub_bd",    as = "pub_bd"   },

      { expr = "app",       as = "app"      },
      { expr = "country",   as = "country"  },
      { expr = "model",      as = "model"     },
      { expr = "brand",     as = "brand"    },
      { expr = "version",   as = "version"  }
    ]
    dwr.groupby.aggs = [
      { expr = "sum(requests)",      as ="requests",          union = "sum(requests)"         },
      { expr = "sum(responses)",     as ="responses",         union = "sum(responses)"        },
      { expr = "sum(loads)",         as ="loads",             union = "sum(loads)"            },
      { expr = "sum(success_loads)", as ="success_loads",     union = "sum(success_loads)"    },
      { expr = "sum(revenue)",       as ="revenue",           union = "sum(revenue)"          },
      { expr = "sum(cost)",          as ="cost",              union = "sum(cost)"             },
      { expr = "0",                  as ="active_users",      union = "sum(active_users)"     },
      { expr = "0",                  as ="new_users",         union = "sum(new_users)"        },
      { expr = "0",                  as ="downloads",         union = "sum(downloads)"        },
      { expr = "0",                  as ="success_downloads", union = "sum(success_downloads)"}
    ]
    dwr.include.repeated = true
    dwr.table = "sdk_dyn_dwr"
    kafka.consumer {
      partitions = [
        {topic = "kafka_topic_dynamic_load"}
      ]
    }
  }

  sdk_dyn_download {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    // timestamp 精确到了秒,非毫秒，所以不用除于1000
    b_time.by = "from_unixtime(timestamp/1000, 'yyyy-MM-dd HH:mm:ss')"
    dwi.enable = true
    dwi.table = "sdk_dyn_download_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SdkDynLoadingDownloadDWISchema"
    dwr.enable = true
    dwr.groupby.fields = [
      { expr = "null",      as = "adv"      },
      { expr = "null",      as = "adv_bd"   },
      { expr = "jar",       as = "jar"      },
      { expr = "null",      as = "pub"      },
      { expr = "null",      as = "pub_bd"   },

      { expr = "app",       as = "app"      },
      { expr = "country",   as = "country"  },
      { expr = "model",     as = "model"   },
      { expr = "brand",     as = "brand"    },
      { expr = "version",   as = "version"  }
    ]
    dwr.groupby.aggs = [
      { expr = "0",                      as ="requests",          union = "sum(requests)"         },
      { expr = "0",                      as ="responses",         union = "sum(responses)"        },
      { expr = "0",                      as ="loads",             union = "sum(loads)"            },
      { expr = "0",                      as ="success_loads",     union = "sum(success_loads)"    },
      { expr = "0",                      as ="revenue",           union = "sum(revenue)"          },
      { expr = "0",                      as ="cost",              union = "sum(cost)"             },
      { expr = "0",                      as ="active_users",      union = "sum(active_users)"     },
      { expr = "0",                      as ="new_users",         union = "sum(new_users)"        },
      { expr = "sum(downloads)",         as ="downloads",         union = "sum(downloads)"        },
      { expr = "sum(success_downloads)", as ="success_downloads", union = "sum(success_downloads)"}
    ]
    dwr.include.repeated = true
    dwr.table = "sdk_dyn_dwr"
    kafka.consumer {
      partitions = [
        {topic = "kafka_topic_jar_download"}
      ]
    }
  }

  sdk_dyn_user_active {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    b_time.by = "from_unixtime(timestamp/1000, 'yyyy-MM-dd HH:mm:ss')"
    dwi.uuid.enable = true
    dwi.uuid.fields = ["imei", "appid"]
    dwi.uuid.b_time.range = [-24, 0]
    dwi.enable = true
    dwi.table = "sdk_dyn_user_active_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SdkDynLoadingUserDWISchema"
    dwr.enable = true
    dwr.groupby.fields = [
      { expr = "null",    as = "adv"      },
      { expr = "null",    as = "adv_bd"   },
      { expr = "null",    as = "jar"      },
      { expr = "null",    as = "pub"      },
      { expr = "null",    as = "pub_bd"   },

      { expr = "appid",   as = "app"      },
      { expr = "null",    as = "country"  },
      { expr = "model",   as = "model"     },
      { expr = "brand",   as = "brand"    },
      { expr = "null",    as = "version"  }
    ]
    dwr.groupby.aggs = [
      { expr = "0",        as ="requests",          union = "sum(requests)"         },
      { expr = "0",        as ="responses",         union = "sum(responses)"        },
      { expr = "0",        as ="loads",             union = "sum(loads)"            },
      { expr = "0",        as ="success_loads",     union = "sum(success_loads)"    },
      { expr = "0",        as ="revenue",           union = "sum(revenue)"          },
      { expr = "0",        as ="cost",              union = "sum(cost)"             },
      { expr = "count(1)", as ="active_users",      union = "sum(active_users)"     },
      { expr = "0",        as ="new_users",         union = "sum(new_users)"        },
      { expr = "0",        as ="downloads",         union = "sum(downloads)"        },
      { expr = "0",        as ="success_downloads", union = "sum(success_downloads)"}
    ]
    dwr.include.repeated = false
    dwr.table = "sdk_dyn_dwr"
    kafka.consumer {
      partitions = [
        {topic = "kafka_topic_sale"}
      ]
    }
  }

  sdk_dyn_user_new {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    b_time.by = "from_unixtime(timestamp/1000, 'yyyy-MM-dd HH:mm:ss')"
    dwi.uuid.enable = true
    dwi.uuid.fields = ["imei", "appid"]
    dwi.uuid.b_time.range = [-720, 0]
    dwi.enable = true
    dwi.table = "sdk_dyn_user_new_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SdkDynLoadingUserDWISchema"
    dwr.enable = true
    dwr.groupby.fields = [
      { expr = "null",        as = "adv"      },
      { expr = "null",        as = "adv_bd"   },
      { expr = "null",        as = "jar"      },
      { expr = "null",        as = "pub"          },
      { expr = "null",        as = "pub_bd"   },

      { expr = "appid",       as = "app"      },
      { expr = "null",        as = "country"  }, // ip 解析得到
      { expr = "model",       as = "model"    },
      { expr = "brand",       as = "brand"    },
      { expr = "null",        as = "version"  }
    ]
    dwr.groupby.aggs = [
      { expr = "0",        as ="requests",          union = "sum(requests)"         },
      { expr = "0",        as ="responses",         union = "sum(responses)"        },
      { expr = "0",        as ="loads",             union = "sum(loads)"            },
      { expr = "0",        as ="success_loads",     union = "sum(success_loads)"    },
      { expr = "0",        as ="revenue",           union = "sum(revenue)"          },
      { expr = "0",        as ="cost",              union = "sum(cost)"             },
      { expr = "0",        as ="active_users",      union = "sum(active_users)"     },
      { expr = "count(1)", as ="new_users",         union = "sum(new_users)"        },
      { expr = "0",        as ="downloads",         union = "sum(downloads)"        },
      { expr = "0",        as ="success_downloads", union = "sum(success_downloads)"}
    ]
    dwr.include.repeated = false
    dwr.table = "sdk_dyn_dwr"
    kafka.consumer {
      partitions = [
        {topic = "kafka_topic_sale"}
      ]
    }
  }

}