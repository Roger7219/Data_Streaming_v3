rdb {
  url = "jdbc:mysql://mysql.noadx.com:3306/sight"
  user = "root"
  password = "MPudKqs79V5ZWPHS"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  jdbc.url = "jdbc:hive2://master:10000/default"
}
message.client.url="http://datarest.noadx.com:5555/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="ssp-kafka001:6667,ssp-kafka002:6667,ssp-kafka003:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "ssp-kafka001:6667,ssp-kafka002:6667,ssp-kafka003:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    //    auto.offset.reset = "earliest"
    auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  set {
    hbase.zookeeper.quorum = "hbase001,hbase002,hbase003"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  streaming.batch.buration = 100
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
    //    spark.streaming.kafka.maxRatePerPartition = 2000
    spark.streaming.kafka.maxRatePerPartition = 1250
    #spark.streaming.receiver.maxRate=1000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 20
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 20
    spark.kryoserializer.buffer.max=256
    spark.scheduler.mode=FAIR
    spark.history.fs.cleaner.enabled = true
    spark.history.fs.cleaner.interval = 1d
    spark.history.fs.cleaner.maxAge = 3d
  }
}
modules {

  smartlink_fill_dwi {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    b_time.by = "createTime"
    dwi.enable = true
    dwi.table = "smartlink_fill_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SspTrafficDWISchema"
    dwr.enable = false
    kafka.consumer {
      partitions = [
        { topic = "topic_ad_fill_new"}
      ]
    }
  }

  smartlink_send_dwi {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    b_time.by = "createTime"
    dwi.enable = true
    dwi.table = "smartlink_send_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SspTrafficDWISchema"
    dwr.enable = false
    kafka.consumer {
      partitions = [
        { topic = "topic_ad_send_new"}
      ]
    }
  }

  smartlink_click_dwi {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    b_time.by = "clickTime"
    dwi.uuid.enable = false
    dwi.uuid.fields = ["clickId"]
    dwi.enable = true
    dwi.table = "smartlink_click_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SspTrafficDWISchema"
    dwr.enable = false
    kafka.consumer {
      partitions = [
        { topic = "topic_ad_click"}
      ]
    }
  }

//因为需要去重，所以放在dwr模块写入
//  smartlink_fee_dwi {
//    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
//    b_time.by = "reportTime"
//    dwi.uuid.enable = true
//    dwi.uuid.fields = ["clickId"]
//    dwi.enable = true
//    dwi.table = "smartlink_fee_dwi"
//    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SspTrafficDWISchema"
//    dwr.enable = false
//    kafka.consumer {
//      partitions = [
//        { topic = "topic_ad_fee"}
//      ]
//    }
//  }


}