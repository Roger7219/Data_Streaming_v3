rdb {
  url = "jdbc:mysql://node17:3306/sight"
  user = "sight"
  password = "%oEL!L#Lkf&B!$F9JapY"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  jdbc.url = "jdbc:hive2://master:10000/default"
}
message.client.url="http://node14:5555/"
kylin.client.url="http://node14:7070/kylin/api/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="node187:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "node187:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    //auto.offset.reset = "earliest"
    //nadx集群有问题，只能用latest
    auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  set {
    hbase.zookeeper.quorum = "master,node187"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  //  app.name = "nadx_dwi"
  streaming.batch.buration = 100
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
    //    spark.streaming.kafka.maxRatePerPartition = 2000
    spark.streaming.kafka.maxRatePerPartition = 5000
    #spark.streaming.receiver.maxRate=1000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 5
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 5
    spark.kryoserializer.buffer=64m
    spark.kryoserializer.buffer.max=512m
    spark.scheduler.mode=FAIR
    //  动态消费速率
    //  spark.streaming.stopGracefullyOnShutdown=true
    //  spark.streaming.backpressure.enabled = true
    //  spark.streaming.backpressure.initialRate = 3000
    //  spark.streaming.backpressure.pid.proportional = 0.5

    //  动态Executor分配
    //  spark.shuffle.service.enabled=true
    //  spark.dynamicAllocation.enabled=true
    //  spark.dynamicAllocation.maxExecutors=6
    //  spark.dynamicAllocation.initialExecutors=1
    //  spark.dynamicAllocation.executorIdleTimeout=20s
    //  spark.dynamicAllocation.cachedExecutorIdleTimeout=20s
    spark.history.fs.cleaner.enabled = true
    spark.history.fs.cleaner.interval = 1d
    spark.history.fs.cleaner.maxAge = 3d
  }
}
modules {
  hivex_traffic_dwi {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    // timestamp 精确到了秒,非毫秒，所以不用除于1000
    business.time.extract.by = "from_unixtime(timestamp, 'yyyy-MM-dd HH:mm:ss')"
    dwi.enable = true
    //    dwi.uuid.enable = true
    //    dwi.uuid.fields = ["dataType", "bidRequestId", "if(demand_id is null, '', demand_id)"]
    dwi.table = "hivex_traffic_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.HiveXTrafficDWISchema"
    //    dwi.business.time.format.by = "yyyy-MM-dd 00:00:00"
    kafka.consumer {
      partitoins = [
        {topic = "hivex_traffic_topic", partition = 0},
        {topic = "hivex_traffic_topic", partition = 1},
        {topic = "hivex_traffic_topic", partition = 2},
        {topic = "hivex_traffic_topic", partition = 3},
        {topic = "hivex_traffic_topic", partition = 4},
        {topic = "hivex_traffic_topic", partition = 5},
        {topic = "hivex_traffic_topic", partition = 6},
        {topic = "hivex_traffic_topic", partition = 7},
        {topic = "hivex_traffic_topic", partition = 8},
        {topic = "hivex_traffic_topic", partition = 9},
        {topic = "hivex_traffic_topic", partition = 10},
        {topic = "hivex_traffic_topic", partition = 11},
        {topic = "hivex_traffic_topic", partition = 12},
        {topic = "hivex_traffic_topic", partition = 13},
        {topic = "hivex_traffic_topic", partition = 14},
        {topic = "hivex_traffic_topic", partition = 15},
        {topic = "hivex_traffic_topic", partition = 16},
        {topic = "hivex_traffic_topic", partition = 17},
        {topic = "hivex_traffic_topic", partition = 18},
        {topic = "hivex_traffic_topic", partition = 19},

        {topic = "hivex_traffic_topic", partition = 0},
        {topic = "hivex_traffic_topic", partition = 1},
        {topic = "hivex_traffic_topic", partition = 2},
        {topic = "hivex_traffic_topic", partition = 3},
        {topic = "hivex_traffic_topic", partition = 4},
        {topic = "hivex_traffic_topic", partition = 5},
        {topic = "hivex_traffic_topic", partition = 6},
        {topic = "hivex_traffic_topic", partition = 7},
        {topic = "hivex_traffic_topic", partition = 8},
        {topic = "hivex_traffic_topic", partition = 9},
        {topic = "hivex_traffic_topic", partition = 10},
        {topic = "hivex_traffic_topic", partition = 11},
        {topic = "hivex_traffic_topic", partition = 12},
        {topic = "hivex_traffic_topic", partition = 13},
        {topic = "hivex_traffic_topic", partition = 14},
        {topic = "hivex_traffic_topic", partition = 15},
        {topic = "hivex_traffic_topic", partition = 16},
        {topic = "hivex_traffic_topic", partition = 17},
        {topic = "hivex_traffic_topic", partition = 18},
        {topic = "hivex_traffic_topic", partition = 19}
      ]
    }
  }

}