rdb {
  url = "jdbc:mysql://node17:3306/sight"
  user = "sight"
  password = "%oEL!L#Lkf&B!$F9JapY"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  jdbc.url = "jdbc:hive2://master:10000/default"
}
message.client.url="http://node14:5555/"
kylin.client.url="http://node14:7070/kylin/api/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="node187:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "node187:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
//    auto.offset.reset = "earliest"
        auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  set {
    hbase.zookeeper.quorum = "master,node187"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  app.name = "hivex_overall"
  streaming.batch.buration = 100
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.default.fileformat=Orc
    hive.exec.dynamic.partition.mode = nonstrict
    //    spark.streaming.kafka.maxRatePerPartition = 2000
    spark.streaming.kafka.maxRatePerPartition = 3000
    #spark.streaming.receiver.maxRate=1000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 80
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 80
    spark.kryoserializer.buffer.max=256
    spark.scheduler.mode=FAIR
    spark.history.fs.cleaner.enabled = true
    spark.history.fs.cleaner.interval = 1d
    spark.history.fs.cleaner.maxAge = 3d
    spark.sql.autoBroadcastJoinThreshold = 209715200
    //    spark.streaming.concurrentJobs = 6
  }
}
modules {
//  site_app_id,
//  placement_id,
//  city,
//  carrier,
//  os_version,
//  device_brand,
//  device_model,
//  bundle
  hivex_overall {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"
    // timestamp 精确到了秒,非毫秒，所以不用除于1000
    business.time.extract.by = "from_unixtime(timestamp, 'yyyy-MM-dd HH:mm:ss')"
    master = true
    dwi.enable = false
//    dwi.table = "nadx_overall_traffic_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.HiveXTrafficDWISchema"
    dwr.enable = true
    dwr.groupby.fields = [
      { expr = "app_key",                as = "app_key"                     },
      { expr = "order_key",              as = "order_key"                   },
      { expr = "adunit_key",             as = "adunit_key"                  },
      { expr = "lineitem_key",           as = "lineitem_key"                },
      { expr = "network_key",            as = "network_key"                 },
      { expr = "appid",                  as = "appid"                       },
      { expr = "cid",                    as = "cid"                         },
      { expr = "city",                   as = "city"                        },
      { expr = "ckv",                    as = "ckv"                         },
      { expr = "country_code",           as = "country_code"                },
      { expr = "cppck",                  as = "cppck"                       },
      { expr = "current_consent_status", as = "current_consent_status"      },
      { expr = "dev",                    as = "dev"                         },
      { expr = "exclude_adgroups",       as = "exclude_adgroups"            },
      { expr = "gdpr_applies",            as = "gdpr_applies"               },
      { expr = "id",                      as = "id"                         },
      { expr = "is_mraid",                as = "is_mraid"                   },
      { expr = "os",                      as = "os"                         },
      { expr = "osv",                     as = "osv"                        },
      { expr = "priority",                as = "priority"                   },
      { expr = "req",                     as = "req"                        },
      { expr = "reqt",                    as = "reqt"                       },
      { expr = "rev",                    as = "rev"                         },
      { expr = "udid",                    as = "udid"                       },
      { expr = "video_type",              as = "video_type"                 }
    ]
    dwr.groupby.aggs = [
      { expr = "sum(request_count)",    as ="request_count",               union = "sum(request_count)"             },
      { expr = "sum(imp_count)",        as ="imp_count",                   union = "sum(imp_count)"                 },
      { expr = "sum(aclk_count)",       as ="aclk_count",                  union = "sum(aclk_count)"                },
      { expr = "sum(attempt_count)",    as ="attempt_count",               union = "sum(attempt_count)"             }
    ]
    dwr.include.repeated = true 
    dwr.table = "hivex_overall_dwr"
    kafka.consumer {
      partitions = [
        {topic = "hivex_traffic_topic"}
      ]
    }
  }
}