rdb {
  url = "jdbc:mysql://node17:3306/sight"
  user = "root"
  password = "root_root"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  jdbc.url = "jdbc:hive2://node17:10000/default"
}
message.client.url="http://node14:5555/"
kylin.client.url="http://node14:7070/kylin/api/"
kafka.producer {
  is.async=true
  set {
    bootstrap.servers="104.250.136.138:6667,104.250.133.18:6667,104.250.130.82:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "node30:6667,node31:6667,node32:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    auto.offset.reset = "earliest"
    #auto.offset.reset = "latest"
    enable.auto.commit = "false"
//    request.timeout.ms = 2000
//    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  transactional {
    tables = ["uuid.stat"]
  }
  set {
    hbase.zookeeper.quorum = "node14,node17,node15"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  app.name = "dw_test"
//  streaming.batch.buration = 300
  streaming.batch.buration = 10
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
    spark.streaming.kafka.maxRatePerPartition = 1
    #spark.streaming.receiver.maxRate=1000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 3
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 3
    spark.kryoserializer.buffer.max=256
    spark.streaming.concurrentJobs = 2
  }
}
modules {

    #请求数，clickid有空值情况
//  ${module.name}
    #请求数
    test {
      class = "com.mobikok.ssp.data.streaming.module.GenericModule"
      business.date.extract.by = "createTime"
      commit.batch.size = 1
      commit.time.interval = 1800
      dwi.fields.extended=[]
      dwi.uuid.enable = false
      dwi.uuid.stat.hbase.table = ""
      dwi.uuid.fields = []
      dwi.uuid.alias = "rowkey"
      dwi.enable = false
      dwi.table = "ssp_test_dwi"
      dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SspTrafficDWISchema"
      dwr.enable = false
      dwr.groupby.fields = [ {
        expr = "publisherId", as = "publisherId"
      }, {
        expr = "subId", as = "subId"
      }, {
        expr = "countryId", as = "countryId"
      }, {
        expr = "carrierId", as = "carrierId"
      },{
        expr = "sv", as = "sv"
      }, {
        expr = "adType", as = "adType"
      }]
      dwr.groupby.aggs = [{
        expr = "count(1)"
        as ="times"
        union = "sum(times)"
      }]
      dwr.table = "ssp_test_dwr"
      dm.kafka.enable = false
      dm.kafka.topic = "topic_ssp_test_dwr"
      //dm.enable = false
      //dm.table = "SSP_FILL_DM_PHOENIX"
      //dm.hbase.storable.class = "com.mobikok.ssp.data.streaming.entity.SspFillDM"


      kafka.consumer {
        partitoins = [
          {
            topic = "topic_ad_fill"
            partition = 0
          }
          ,
          {
            topic = "topic_ad_fill"
            partition = 1
          }
        ]
        set {
          bootstrap.servers = "104.250.136.138:6667"
          group.id = "test_topic_ad_fill_dwi51_group1"

        }
      }
    }

//    test22 {
//      class = "com.mobikok.ssp.data.streaming.module.GenericModule"
//      business.date.extract.by = "createTime"
//      commit.batch.size = 1
//      commit.time.interval = 1800
//      dwi.fields.extended=[]
//      dwi.uuid.enable = false
//      dwi.uuid.stat.hbase.table = ""
//      dwi.uuid.fields = []
//      dwi.uuid.alias = "rowkey"
//      dwi.enable = true
//      dwi.table = "ssp_test22_dwi"
//      dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.SspTrafficDWISchema"
//      dwr.enable = true
//      dwr.groupby.fields = [ {
//        expr = "publisherId", as = "publisherId"
//      }, {
//        expr = "subId", as = "subId"
//      }, {
//        expr = "countryId", as = "countryId"
//      }, {
//        expr = "carrierId", as = "carrierId"
//      },{
//        expr = "sv", as = "sv"
//      }, {
//        expr = "adType", as = "adType"
//      }]
//      dwr.groupby.aggs = [{
//        expr = "count(1)"
//        as ="times"
//        union = "sum(times)"
//      }]
//      dwr.include.repeated = true
//      dwr.table = "ssp_test22_dwr"
//      dm.kafka.enable = false
//      dm.kafka.topic = "topic_ssp_test22_dwr"
//      //dm.enable = false
//      //dm.table = "SSP_FILL_DM_PHOENIX"
//      //dm.hbase.storable.class = "com.mobikok.ssp.data.streaming.entity.SspFillDM"
//      kafka.consumer {
//        partitoins = [
//          {
//            topic = "topic_ad_fill"
//            partition = 0
//          },
//          {
//            topic = "topic_ad_click"
//            partition = 0
//          }
//        ]
//        set {
//          bootstrap.servers = "node30:6667,node31:6667,node32:6667"
//          key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
//          value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
//          #?
//          group.id = "test22_topic_ad_fill_dwi_group1"
//          auto.offset.reset = "earliest"
//          #auto.offset.reset = "latest"
//          enable.auto.commit = "false"
//          request.timeout.ms = 2000
//          session.timeout.ms = 1500
//          heartbeat.interval.ms = 1000
//        }
//      }
//    }
}