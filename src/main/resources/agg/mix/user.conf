rdb {
  url = "jdbc:mysql://node17:3306/sight"
  user = "root"
  password = "root_root"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  # transactional {
  #    tables = ["ad_click_dwi", "ad_fee_dwi"]
  #  }
  jdbc.url = "jdbc:hive2://node17:10000/default"
}
message.client.url="http://node14:5555/"
kylin.client.url="http://node14:7070/kylin/api/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="node30:6667,node31:6667,node32:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "node30:6667,node31:6667,node32:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    auto.offset.reset = "earliest"
    #auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  set {
    hbase.zookeeper.quorum = "node106,node107,node108"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  app.name = "agg_user"
//  streaming.batch.buration = 200
  streaming.batch.buration = 600
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
    spark.streaming.kafka.maxRatePerPartition = 1000
//    spark.streaming.kafka.maxRatePerPartition = 3000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 3
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 3
    spark.kryoserializer.buffer.max=512
//    spark.kryo.registrationRequired = true
    spark.streaming.concurrentJobs = 3
  }
}
modules {

  agg_user_new {
    class = "com.mobikok.ssp.data.streaming.module.MixModule"
    business.date.extract.by = "createTime"
    commit.time.interval = 1800
    commit.batch.size = 1
    dwi.uuid.enable = true
    dwi.uuid.stat.hbase.table = "agg_user_new_dwi__uuid"
    dwi.uuid.fields = ["appId", "jarId", "imei"]
    dwi.uuid.alias = "rowkey"
    dwi.enable = true
    dwi.table = "agg_user_dwi"
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.AggUserDWISchema"
    #dwr.groupby.fields = ["appId", "countryId", "carrierId", "sv"]
    dwr.enable = true
    dwr.groupby.fields = [{
      expr = "jarId",       as = "jarId"
    }, {
      expr = "appId",       as = "appId"
    }, {
      expr = "countryId",   as = "countryId"
    }, {
      expr = "carrierId",   as = "carrierId"
    },{
      expr = "connectType", as = "connectType"
    },{
      expr = "publisherId", as = "publisherId"
    },{
      expr = "affSub",      as = "affSub"
    }]
    dwr.groupby.aggs = [{
      expr = "count( if(repeated = 'N', repeated, null) )"
      as ="newCount"
      union = "sum(newCount)"
    }]
    dwr.table = "agg_user_new_dwr"
    dm.kafka.enable = true
    dm.kafka.topic = "topic_agg_user_new_dwr"
    kafka.consumer {
      partitoins = [
        {
          topic = "kok_inter_user_info"
          partition = 0
        }
      ]
    }
  }
  agg_user_active {
    class = "com.mobikok.ssp.data.streaming.module.MixModule"
    business.date.extract.by = "createTime"
    commit.time.interval = 1800
    commit.batch.size = 1
    dwi.uuid.enable = true
    dwi.uuid.stat.hbase.table = "agg_user_active_dwi__uuid"
    dwi.uuid.fields = ["appId", "jarId", "imei"]
    dwi.enable = false         // 仅用于去重，明细数据由agg_user_new模块来保存
    dwi.table = "agg_user_dwi" // 去重需要读这个明细表
    dwi.handler = [{
      expr = "", as = [
        "dayRepeated"
        "appId",
        "jarId",
        "jarIds",
        "publisherId",
        "imei",
        "imsi",
        "version",
        "model",
        "screen",
        "installType",
        "sv",
        "leftSize",
        "androidId",
        "userAgent",
        "connectType",
        "createTime",
        "countryId",
        "carrierId",
        "ipAddr",
        "deviceType",
        "pkgName",
        "affSub"
      ]
      handler = {
        class = "com.mobikok.ssp.data.streaming.handler.dwi.AggUserActiveHandler",
        uuid.table = "agg_user_active_dwi__day_uuid"
      }
    }]
    dwi.kafka.schema = "com.mobikok.ssp.data.streaming.schema.dwi.kafka.AggUserDWISchema"
    dwr.enable = true
    dwr.groupby.fields = [{
      expr = "jarId",       as = "jarId"
    }, {
      expr = "appId",       as = "appId"
    }, {
      expr = "countryId",   as = "countryId"
    }, {
      expr = "carrierId",   as = "carrierId"
    },{
      expr = "connectType", as = "connectType"
    },{
      expr = "publisherId", as = "publisherId"
    },{
      expr = "affSub",      as = "affSub"
    }]
    dwr.groupby.aggs = [{
      expr = "count( if(repeats >= 0 and dayRepeated = 'N', repeats, null) ) "
      as ="activeCount"
      union = "sum(activeCount)"
    }]
    dwr.table = "agg_user_active_dwr"
    dm.kafka.enable = true
    dm.kafka.topic = "topic_agg_user_active_dwr"
    kafka.consumer {
      partitoins = [
        {
          topic = "kok_inter_user_info"
          partition = 0
        }
      ]
    }
  }
}