rdb {
  url = "jdbc:mysql://node17:3306/sight"
  user = "root"
  password = "root_root"
  kafka.offset.table = "offset"
  transaction.manager.table="sight.transaction_commited_table"
}
hive {
  jdbc.url = "jdbc:hive2://node17:10000/default"
}
message.client.url="http://node14:5555/"
kylin.client.url="http://node14:7070/kylin/api/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="node30:6667,node31:6667,node32:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "node30:6667,node31:6667,node32:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    auto.offset.reset = "earliest"
    #auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}
hbase {
  transactional {
    tables = ["uuid.stat"]
  }
  set {
    hbase.zookeeper.quorum = "node106,node107,node108"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  app.name = "ssp_mix"
  streaming.batch.buration = 100
//  streaming.batch.buration = 10
  set {
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
//    spark.streaming.backpressure.enabled = true
    spark.streaming.kafka.maxRatePerPartition = 6000
//    spark.streaming.kafka.maxRatePerPartition = 60000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 3
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 3
    spark.kryoserializer.buffer.max=512
//    spark.kryo.registrationRequired = true
//    spark.streaming.concurrentJobs = 5
    spark.scheduler.mode=FAIR
  }
}
ref.modules = [
  // "bq_dupscribe.conf", +临时
//  "dsp.conf"
//  "dupscribe.conf" ,//和dsp模块 jar冲突
//  "fee.conf"
  //  ,"fill.conf" +临时
  //  ,"image.conf"
  //,
  "log.conf"
  //  ,"send2.conf"
  //  ,"show.conf"
  ,"user.conf",
//  "smartdatavo.conf",
//  "programmed_bid.conf",
  "postback.conf"
]




