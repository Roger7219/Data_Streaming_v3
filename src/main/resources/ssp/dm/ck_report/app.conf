rdb {
  url = "jdbc:mysql://node17:3306/sight?autoReconnect=true&failOverReadOnly=false"
  user = "sight"
  password = "%oEL!L#Lkf&B!$F9JapY"
  kafka.offset.table = "offset"
}
hive {
  jdbc.url = "jdbc:hive2://node17:10000/default"
}
message.client.url="http://node14:5555/"
kafka.producer {
  is.async=false
  set {
    bootstrap.servers="node30:6667,node104:6667,node32:6667"
    client.id="niubility_producer"
    acks=-1
    key.serializer="org.apache.kafka.common.serialization.StringSerializer"
    value.serializer="org.apache.kafka.common.serialization.StringSerializer"
  }
}
kafka.consumer {
  set {
    bootstrap.servers = "node30:6667,node104:6667,node32:6667"
    key.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    value.deserializer = "org.apache.kafka.common.serialization.StringDeserializer"
    //    auto.offset.reset = "earliest"
    auto.offset.reset = "latest"
    enable.auto.commit = "false"
    request.timeout.ms = 2000
    session.timeout.ms = 1500
    heartbeat.interval.ms = 1000
  }
}

hbase {
  set {
    hbase.zookeeper.quorum = "node14,node15,node16,node17,node30,node32,node124,node125,node126"
    hbase.zookeeper.property.clientPort = "2181"
    spark.serializer = org.apache.spark.serializer.KryoSerializer
  }
}
spark.conf {
  streaming.batch.buration = 200
  //  streaming.batch.buration = 10
  set {
//    spark.app.name = "bq_mix"
    mapreduce.job.queuename = queueA
    mapreduce.job.priority = HIGH
    hive.exec.dynamic.partition.mode = nonstrict
    //    spark.streaming.backpressure.enabled = true
    spark.streaming.kafka.maxRatePerPartition = 1000
    //    spark.streaming.kafka.maxRatePerPartition = 60000
    spark.serializer = org.apache.spark.serializer.KryoSerializer
    spark.default.parallelism = 3
    hive.merge.mapfiles = true
    hive.merge.mapredfiles = true
    hive.merge.smallfiles.avgsize=1024000000
    spark.sql.shuffle.partitions = 3
    spark.kryoserializer.buffer.max=512
    //    spark.kryo.registrationRequired = true
    spark.streaming.concurrentJobs = 16
    spark.scheduler.mode=FAIR
    spark.sql.broadcastTimeout=3000
  }
}
clickhouse {
  hosts = ["node111", "node110", "node16" , "node15"]
}
modules= {

  ck_report_overall {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"

    dm.handlers = [{
      class = "com.mobikok.ssp.data.streaming.handler.dm.ClickHouseUploadByBTimeHandler"
      items = [{
        view = "ssp_report_overall_dm"
        ck = "ssp_report_overall_dm"
        message.consumer = "ck_report_overall_cer"
        message.topics = [
          "ssp_report_overall_dwr",
          "update_ssp_report_overall2",
          "PublisherThirdIncomeDMReflush"
        ]
      }]
    }]
  }

  ck_report_overall_day {
    class = "com.mobikok.ssp.data.streaming.module.PluggableModule"

    dm.handlers = [{
      class = "com.mobikok.ssp.data.streaming.handler.dm.ClickHouseUploadByBTimeHandler"
      items = [{
        // 注意：这里配置的hive视图名和clickhouse表名不是一样的！
        view = "ssp_report_overall_dm_day_v2"
        ck = "ssp_report_overall_dm_day"
        message.consumer = "ck_report_overall_day_cer"
        message.topics = [
          "ssp_report_overall_dwr_day",
          "ssp_report_overall_dm_day_v2_update",
          "PublisherThirdIncomeDMReflush"
        ]
      }]
    }]
  }

}
